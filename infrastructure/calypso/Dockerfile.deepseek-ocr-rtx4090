FROM pytorch/pytorch:2.3.0-cuda12.1-cudnn8-devel

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    curl \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Upgrade to PyTorch 2.6.0 and install DeepSeek-OCR dependencies
RUN pip install --no-cache-dir \
    torch==2.6.0 \
    torchvision==0.21.0 \
    torchaudio==2.6.0 \
    transformers==4.46.3 \
    tokenizers==0.20.3 \
    && rm -rf /root/.cache/pip

# Install vLLM 0.8.5 (the version that works with their approach)
RUN pip install --no-cache-dir \
    vllm==0.8.5 \
    && rm -rf /root/.cache/pip

# Install flash-attn and other dependencies
RUN pip install --no-cache-dir \
    flash-attn==2.7.3 --no-build-isolation \
    PyMuPDF \
    img2pdf \
    einops \
    easydict \
    addict \
    Pillow \
    numpy \
    && rm -rf /root/.cache/pip

# Clone the DeepSeek-OCR repository
WORKDIR /workspace
RUN git clone https://github.com/deepseek-ai/DeepSeek-OCR.git

# Set working directory to the vLLM scripts
WORKDIR /workspace/DeepSeek-OCR/DeepSeek-OCR-master/DeepSeek-OCR-vllm

# Set environment variables for RTX 4090
ENV VLLM_USE_V1=0
ENV CUDA_VISIBLE_DEVICES=0
ENV VLLM_FLASH_ATTN_VERSION=2

# Expose port for potential API server
EXPOSE 8000

# Default command - start a bash shell so we can run scripts interactively
CMD ["/bin/bash"]
