name: Train and Publish NER Model
# ESC-powered workflow

on:
  workflow_dispatch:
    inputs:
      trigger_source:
        description: "Source of dispatch (optional)"
        required: false
        default: "argilla"
  schedule:
    - cron: '0 3 * * *'  # daily at 03:00 UTC

permissions:
  contents: read

jobs:
  train-export-publish:
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup micromamba
        uses: mamba-org/setup-micromamba@v1
        with:
          micromamba-version: 'latest'
          init-shell: bash
          cache-downloads: true
          cache-environment: false

      - name: Authenticate to Pulumi Cloud (OIDC)
        uses: pulumi/auth-actions@v1
        with:
          organization: ryan-taylor
          requested-token-type: urn:pulumi:token-type:access_token:personal
          scope: user:ryan-taylor

      - name: Export HF token from ESC
        id: esc
        run: |
          # Install ESC CLI
          curl -fsSL https://get.pulumi.com/esc/install.sh | sh
          export PATH="$HOME/.pulumi/bin:$PATH"

          # Get HF token from ESC (camelCase key to match Pulumi config)
          HF_TOKEN=$(esc env get default/oceanid-cluster pulumiConfig.oceanid-cluster:hfAccessToken --show-secrets --value string)
          if [ -z "$HF_TOKEN" ] || [ "$HF_TOKEN" = "[secret]" ]; then
            echo "âŒ HF token not found in ESC (pulumiConfig.oceanid-cluster:hfAccessToken)" >&2
            exit 1
          fi
          echo "::add-mask::$HF_TOKEN"
          echo "HF_TOKEN=$HF_TOKEN" >> "$GITHUB_ENV"

          # Get HF repos from ESC (with sensible defaults)
          HF_DATASET_REPO=$(esc env get default/oceanid-cluster pulumiConfig.oceanid-cluster:hfDatasetRepo --value string 2>/dev/null || echo "goldfish-inc/oceanid-annotations")
          HF_DATASET_REPO_NER=$(esc env get default/oceanid-cluster pulumiConfig.oceanid-cluster:hfDatasetRepoNER --value string 2>/dev/null || echo "")
          HF_MODEL_REPO=$(esc env get default/oceanid-cluster pulumiConfig.oceanid-cluster:hfModelRepo --value string 2>/dev/null || echo "goldfish-inc/oceanid-ner-distilbert")
          if [ -z "$HF_DATASET_REPO" ]; then HF_DATASET_REPO="goldfish-inc/oceanid-annotations"; fi
          if [ -z "$HF_MODEL_REPO" ]; then HF_MODEL_REPO="goldfish-inc/oceanid-ner-distilbert"; fi
          # Prefer NER-specific dataset repo if provided
          if [ -n "$HF_DATASET_REPO_NER" ]; then HF_DATASET_REPO="$HF_DATASET_REPO_NER"; fi
          echo "HF_DATASET_REPO=$HF_DATASET_REPO" >> "$GITHUB_ENV"
          echo "HF_MODEL_REPO=$HF_MODEL_REPO" >> "$GITHUB_ENV"

      - name: Create env
        run: |
          micromamba env create -f apps/ner-training/environment.yml -y
          micromamba run -n oceanid-ner-py311 python -c "import transformers, torch; print('TF', transformers.__version__, 'Torch', torch.__version__)"

      - name: Fetch HF dataset annotations
        env:
          HF_TOKEN: ${{ env.HF_TOKEN }}
          HF_DATASET_REPO: ${{ env.HF_DATASET_REPO }}
        run: |
          micromamba run -n oceanid-ner-py311 python - << 'PY'
          import os, json, pathlib
          from huggingface_hub import HfApi, hf_hub_download, list_repo_files
          HF_TOKEN=os.environ['HF_TOKEN']
          REPO=os.environ.get('HF_DATASET_REPO','goldfish-inc/oceanid-annotations')
          api=HfApi(token=HF_TOKEN)
          shards=pathlib.Path('shards'); shards.mkdir(exist_ok=True)
          files=list_repo_files(REPO, repo_type='dataset')
          count=0
          for f in files:
              # Outbox shards pattern
              if f.startswith('schema-') or f.startswith('vertical='):
                  if f.endswith('.jsonl'):
                      p=hf_hub_download(REPO, filename=f, repo_type='dataset', token=HF_TOKEN)
                      dst=shards/ pathlib.Path(f.replace('/','__')).name
                      with open(p,'rb') as src, open(dst,'wb') as out:
                          out.write(src.read())
                      count+=1
          print('downloaded shard jsonl files to', shards, 'count=', count)

          # Normalize into training format {"text": str, "spans": [{start,end,label},...]}
          norm=pathlib.Path('local_annotations'); norm.mkdir(exist_ok=True)
          out_path=norm/'ner.jsonl'
          with open(out_path,'w',encoding='utf-8') as outf:
              for p in shards.glob('*.jsonl'):
                  with open(p,'r',encoding='utf-8') as f:
                      for line in f:
                          line=line.strip()
                          if not line:
                              continue
                          try:
                              rec=json.loads(line)
                          except Exception:
                              continue
                          # annotationRecord payload
                          ann = rec.get('annotation') or {}
                          task_data = rec.get('task_data') or {}
                          text = ''
                          if isinstance(task_data, dict):
                              text = task_data.get('text') or ''
                          spans=[]
                          try:
                              results = ann.get('result') or []
                              for r in results:
                                  if not isinstance(r, dict):
                                      continue
                                  if r.get('type') not in ('labels','choices'):
                                      continue
                                  val = r.get('value') or {}
                                  if all(k in val for k in ('start','end','labels')):
                                      label_list = val.get('labels') or []
                                      label = label_list[0] if label_list else None
                                      if label is None:
                                          continue
                                      spans.append({'start': int(val.get('start',0)), 'end': int(val.get('end',0)), 'label': str(label)})
                          except Exception:
                              pass
                          if text and spans:
                              outf.write(json.dumps({'text': text, 'spans': spans}, ensure_ascii=False) + '\n')
          print('normalized training data written to', out_path)
          PY

      - name: Train NER
        env:
          TOKENIZERS_PARALLELISM: 'false'
        run: |
          micromamba run -n oceanid-ner-py311 python scripts/ner_train.py --labels labels.json --data-dir ./local_annotations --out ./models/ner-distilbert

      - name: Export ONNX (opset 14)
        env:
          TOKENIZERS_PARALLELISM: 'false'
        run: |
          micromamba run -n oceanid-ner-py311 python apps/ner-training/export_onnx.py \
            --model ./models/ner-distilbert \
            --output ./distilbert_onnx \
            --opset 14

      - name: Write VERSION.json
        run: |
          micromamba run -n oceanid-ner-py311 python - << 'PY'
          import json, time, os
          import onnx
          import transformers, torch, onnxruntime
          out_dir = 'distilbert_onnx'
          mode = 'unknown'
          try:
              mode = open(os.path.join(out_dir,'exporter_mode.txt'),'r').read().strip()
          except Exception:
              pass
          opset = None
          try:
              m = onnx.load(os.path.join(out_dir,'model.onnx'))
              if m.opset_import:
                  opset = int(m.opset_import[0].version)
          except Exception:
              pass
          labels = []
          try:
              lm = json.load(open(os.path.join(out_dir,'labels.json'),'r'))
              id2 = {int(k):v for k,v in lm.get('id2label', {}).items()}
              labels = [id2[i] for i in sorted(id2.keys())]
          except Exception:
              pass
          ver = {
              'exporter_mode': mode,
              'versions': {
                  'transformers': transformers.__version__,
                  'torch': torch.__version__,
                  'onnxruntime': onnxruntime.__version__,
                  'onnx': onnx.__version__,
              },
              'opset': opset,
              'labels': labels,
              'timestamp_utc': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
          }
          with open(os.path.join(out_dir,'VERSION.json'),'w') as f:
              json.dump(ver, f, indent=2)
          print('Wrote', os.path.join(out_dir,'VERSION.json'))
          PY

      - name: Add Triton config
        run: |
          cp apps/ner-training/config.pbtxt.template distilbert_onnx/config.pbtxt

      - name: Check ONNX size and report
        run: |
          set -euo pipefail
          FILE="distilbert_onnx/model.onnx"
          if [ ! -f "$FILE" ]; then echo "ONNX missing" >&2; exit 1; fi
          BYTES=$(wc -c < "$FILE" | tr -d ' ')
          MB=$(awk -v b="$BYTES" 'BEGIN{printf "%.2f", b/1024/1024}')
          echo "ONNX size: ${MB} MB (${BYTES} bytes)"
          THRESHOLD=$((80*1024*1024)) # 80 MB
          if [ "$BYTES" -gt "$THRESHOLD" ]; then
            echo "::warning title=ONNX size::Model size ${MB} MB exceeds 80 MB threshold"
          fi
          echo "ONNX_BYTES=${BYTES}" >> $GITHUB_ENV

      - name: Collect training metrics
        run: |
          set -euo pipefail
          METRICS_PATH="./models/ner-distilbert/metrics.json"
          if [ -f "$METRICS_PATH" ]; then
            echo "Found metrics at $METRICS_PATH";
          else
            echo '{}' > "$METRICS_PATH"
          fi
          echo "METRICS_PATH=$METRICS_PATH" >> $GITHUB_ENV

      - name: Publish metrics to HF model repo
        if: success()
        env:
          HF_TOKEN: ${{ env.HF_TOKEN }}
          HF_MODEL_REPO: ${{ env.HF_MODEL_REPO }}
          ONNX_BYTES: ${{ env.ONNX_BYTES }}
          METRICS_PATH: ${{ env.METRICS_PATH }}
        run: |
          micromamba run -n oceanid-ner-py311 python - << 'PY'
          import os, io, json
          from datetime import datetime
          from huggingface_hub import HfApi, CommitOperationAdd
          repo=os.environ['HF_MODEL_REPO']
          api=HfApi(token=os.environ['HF_TOKEN'])
          api.create_repo(repo_id=repo, repo_type='model', private=True, exist_ok=True)
          ts=datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')
          metrics_path=os.environ.get('METRICS_PATH','./models/ner-distilbert/metrics.json')
          try:
            metrics=json.load(open(metrics_path,'r'))
          except Exception:
            metrics={}
          metrics['onnx_bytes']=int(os.environ.get('ONNX_BYTES','0') or 0)
          data=json.dumps(metrics, ensure_ascii=False, indent=2).encode('utf-8')
          op=CommitOperationAdd(path_in_repo=f'metrics/ner_{ts}.json', path_or_fileobj=io.BytesIO(data))
          api.create_commit(repo_id=repo, repo_type='model', operations=[op], commit_message=f"Add metrics {ts}")
          print('published metrics to', repo)
          PY

      - name: Publish ONNX to HF model repo
        env:
          HF_TOKEN: ${{ env.HF_TOKEN }}
          HF_MODEL_REPO: ${{ env.HF_MODEL_REPO }}
        run: |
          micromamba run -n oceanid-ner-py311 python - << 'PY'
          import os, io
          from datetime import datetime
          from huggingface_hub import HfApi, CommitOperationAdd
          repo=os.environ.get('HF_MODEL_REPO','goldfish-inc/oceanid-ner-distilbert')
          api=HfApi(token=os.environ['HF_TOKEN'])
          api.create_repo(repo_id=repo, repo_type='model', private=True, exist_ok=True)
          bin=open('distilbert_onnx/model.onnx','rb').read()
          op=CommitOperationAdd(path_in_repo='onnx/model.onnx', path_or_fileobj=io.BytesIO(bin))
          cfg=open('distilbert_onnx/config.pbtxt','rb').read()
          op_cfg=CommitOperationAdd(path_in_repo='onnx/config.pbtxt', path_or_fileobj=io.BytesIO(cfg))
          ver=open('distilbert_onnx/VERSION.json','rb').read()
          op_ver=CommitOperationAdd(path_in_repo='onnx/VERSION.json', path_or_fileobj=io.BytesIO(ver))
          api.create_commit(repo_id=repo, repo_type='model', operations=[op, op_cfg, op_ver], commit_message=f"Update ONNX {datetime.utcnow().isoformat()}Z")
          print('published ONNX to', repo)
          PY
