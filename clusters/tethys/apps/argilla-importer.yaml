---
# ConfigMap containing the import script
apiVersion: v1
kind: ConfigMap
metadata:
  name: argilla-import-scripts
  namespace: apps
data:
  import_datasets.py: |
    #!/usr/bin/env python3
    """
    Server-side Argilla dataset importer
    Imports OCR datasets from Hugging Face into Argilla for annotation
    """
    import os
    import sys
    import argilla as rg
    from datasets import load_dataset
    from huggingface_hub import HfApi

    # Configuration from environment
    ARGILLA_API_URL = os.environ["ARGILLA_API_URL"]
    ARGILLA_API_KEY = os.environ["ARGILLA_API_KEY"]
    HF_TOKEN = os.environ["HF_TOKEN"]
    DATASET_ID = os.environ.get("HF_DATASET_ID", "goldfish-inc/deepseekocr-output")
    WORKSPACE = os.environ.get("ARGILLA_WORKSPACE", "argilla")
    ARGILLA_DATASET_NAME = os.environ.get("ARGILLA_DATASET_NAME", "deepseekocr-output")

    print(f"üöÄ Argilla Dataset Importer")
    print(f"   API URL: {ARGILLA_API_URL}")
    print(f"   HF Dataset: {DATASET_ID}")
    print(f"   Argilla Dataset: {ARGILLA_DATASET_NAME}")
    print(f"   Workspace: {WORKSPACE}")

    # Verify HF access
    print(f"\nüîç Verifying Hugging Face access...")
    api = HfApi(token=HF_TOKEN)
    user = api.whoami()
    print(f"‚úì Authenticated as: {user['name']}")

    try:
        dataset_info = api.dataset_info(DATASET_ID)
        print(f"‚úì Dataset accessible: {DATASET_ID}")
        print(f"  Private: {dataset_info.private}")
    except Exception as e:
        print(f"‚ùå Cannot access dataset: {e}")
        sys.exit(1)

    # Connect to Argilla
    print(f"\nüîå Connecting to Argilla...")
    client = rg.Argilla(api_url=ARGILLA_API_URL, api_key=ARGILLA_API_KEY)
    print(f"‚úì Connected")

    # Load dataset from HF
    print(f"\nüì• Loading dataset from Hugging Face...")
    try:
        hf_dataset = load_dataset(DATASET_ID, token=HF_TOKEN)
        first_split = list(hf_dataset.keys())[0]
        total_records = len(hf_dataset[first_split])
        print(f"‚úì Dataset loaded: {total_records} records")
    except Exception as e:
        print(f"‚ùå Failed to load dataset: {e}")
        sys.exit(1)

    # Create or get Argilla dataset
    print(f"\nüìã Creating Argilla dataset schema...")

    # Define NER dataset settings for vessel intelligence extraction
    settings = rg.Settings(
        guidelines="Extract vessel and maritime regulatory information from OCR'd IUU fishing documents. Focus on accurate identification of vessel identifiers, ownership, compliance status, and regulatory details.",
        fields=[
            rg.TextField(
                name="text",
                title="Document Text",
                description="OCR-extracted text from PDF page",
                use_markdown=False
            ),
            rg.TextField(
                name="document_id",
                title="Source Document ID",
                description="Identifier: {pdf_name}_page_{page_number}",
                use_markdown=False
            ),
        ],
        questions=[
            rg.SpanQuestion(
                name="entities",
                title="Label all vessel intelligence entities",
                description="Select text spans and assign entity types",
                labels=[
                    # Core Identifiers
                    "VESSEL_NAME",
                    "IMO_NUMBER",
                    "MMSI",
                    "IRCS_CALL_SIGN",
                    "FLAG_STATE",
                    "NATIONAL_REGISTRY_NUMBER",
                    "EU_CFR_NUMBER",
                    # Vessel Specifications
                    "VESSEL_TYPE",
                    "TONNAGE",
                    "LENGTH",
                    "ENGINE_POWER",
                    "BUILD_YEAR",
                    "BUILDER_NAME",
                    "HULL_NUMBER",
                    # Ownership & Operation
                    "OWNER_NAME",
                    "OWNER_ADDRESS",
                    "OPERATOR_NAME",
                    "BENEFICIAL_OWNER",
                    "CHARTER_COMPANY",
                    "REGISTRATION_PORT",
                    # Compliance & Authorization
                    "RFMO_NAME",
                    "AUTHORIZATION_NUMBER",
                    "LICENSE_NUMBER",
                    "PERMIT_TYPE",
                    "VALIDITY_PERIOD",
                    "AUTHORIZED_AREA",
                    "AUTHORIZED_SPECIES",
                    # Watchlist & Risk
                    "IUU_LISTING",
                    "SANCTION_TYPE",
                    "VIOLATION_TYPE",
                    "DETENTION_PORT",
                    "INSPECTION_DATE",
                    # Species & Catch
                    "SPECIES_NAME",
                    "SPECIES_CODE",
                    "CATCH_QUANTITY",
                    "CATCH_UNIT",
                    "FISHING_GEAR_TYPE",
                    # Historical Events
                    "PREVIOUS_NAME",
                    "PREVIOUS_FLAG",
                    "NAME_CHANGE_DATE",
                    "FLAG_CHANGE_DATE",
                    "OWNERSHIP_TRANSFER_DATE",
                    # Geographic & Temporal
                    "PORT_NAME",
                    "COORDINATES",
                    "DATE",
                    "REPORTING_PERIOD",
                    # Organizations & Officials
                    "GOVERNMENT_AGENCY",
                    "INSPECTION_AUTHORITY",
                    "CERTIFYING_BODY",
                    "OFFICIAL_NAME",
                    "OFFICIAL_TITLE",
                ],
                required=True
            ),
        ],
        metadata=[
            rg.TermsMetadataProperty(name="pdf_name", title="Source PDF"),
            rg.IntegerMetadataProperty(name="page_number", title="Page Number"),
            rg.TermsMetadataProperty(name="timestamp", title="OCR Processing Time"),
        ],
    )

    # Create dataset
    try:
        argilla_dataset = rg.Dataset(
            name=ARGILLA_DATASET_NAME,
            workspace=WORKSPACE,
            settings=settings,
        )
        argilla_dataset.create()
        print(f"‚úì Dataset '{ARGILLA_DATASET_NAME}' created")
    except Exception as e:
        if "already exists" in str(e).lower():
            print(f"‚ÑπÔ∏è  Dataset '{ARGILLA_DATASET_NAME}' already exists, using existing")
            argilla_dataset = client.datasets(name=ARGILLA_DATASET_NAME, workspace=WORKSPACE)
        else:
            print(f"‚ùå Failed to create dataset: {e}")
            sys.exit(1)

    # Convert HF records to Argilla records
    print(f"\nüì§ Converting and uploading records...")
    argilla_records = []

    for idx, hf_record in enumerate(hf_dataset[first_split]):
        # Create document_id from pdf_name and page_number
        document_id = f"{hf_record['pdf_name']}_page_{hf_record['page_number']}"

        argilla_record = rg.Record(
            fields={
                "text": hf_record["clean_text"] or hf_record["text"],
                "document_id": document_id,
            },
            metadata={
                "pdf_name": hf_record["pdf_name"],
                "page_number": hf_record["page_number"],
                "timestamp": hf_record["timestamp"],
            },
        )
        argilla_records.append(argilla_record)

        # Progress indicator
        if (idx + 1) % 50 == 0:
            print(f"  Converted {idx + 1}/{total_records} records...")

    # Upload to Argilla
    print(f"\n‚¨ÜÔ∏è  Uploading {len(argilla_records)} records to Argilla...")
    try:
        argilla_dataset.records.log(argilla_records)
        print(f"‚úÖ Successfully imported {len(argilla_records)} records!")
        print(f"\nüéâ Dataset ready for NER annotation at:")
        print(f"   {ARGILLA_API_URL}/dataset/{ARGILLA_DATASET_NAME}/annotation-mode")
        print(f"\nüìù Next steps:")
        print(f"   1. Optionally add pre-annotations from Ollama Worker outputs")
        print(f"   2. Start manual annotation of vessel entities")
        print(f"   3. Export training data for spaCy NER model")
    except Exception as e:
        print(f"‚ùå Failed to upload records: {e}")
        sys.exit(1)

---
# CronJob for scheduled imports (disabled by default)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: argilla-dataset-importer
  namespace: apps
spec:
  schedule: "0 2 * * *"  # Daily at 2am UTC
  suspend: true  # Start disabled - enable when ready
  jobTemplate:
    metadata:
      labels:
        app: argilla-importer
    spec:
      template:
        metadata:
          labels:
            app: argilla-importer
        spec:
          restartPolicy: OnFailure
          containers:
          - name: importer
            image: python:3.11-slim
            command: ["/bin/bash", "-c"]
            args:
            - |
              set -e
              echo "üì¶ Installing dependencies..."
              pip install --quiet argilla datasets huggingface_hub
              echo "‚úì Dependencies installed"
              echo ""
              python /scripts/import_datasets.py
            env:
            - name: ARGILLA_API_URL
              value: "http://argilla:6900"
            - name: ARGILLA_API_KEY
              valueFrom:
                secretKeyRef:
                  name: argilla-secrets
                  key: ADMIN_API_KEY
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: argilla-secrets
                  key: HF_TOKEN
            - name: HF_DATASET_ID
              value: "goldfish-inc/deepseekocr-output"
            - name: ARGILLA_WORKSPACE
              value: "argilla"
            volumeMounts:
            - name: import-scripts
              mountPath: /scripts
            resources:
              requests:
                cpu: 100m
                memory: 256Mi
              limits:
                cpu: 500m
                memory: 1Gi
          volumes:
          - name: import-scripts
            configMap:
              name: argilla-import-scripts
              defaultMode: 0755
